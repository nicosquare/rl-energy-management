{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from src.environments.simple_microgrid import SimpleMicrogrid\n",
    "from src.utils.tools import set_all_seeds, load_config, plot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "config = load_config(\"zero_mg\")\n",
    "\n",
    "env = SimpleMicrogrid(config=config['env'])\n",
    "\n",
    "all_states_za_train, all_rewards_za_train, all_actions_za_train, all_net_energy_za_train = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_za_train, rewards_za_train, actions_za_train= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_za_train.append(state_0)\n",
    "    # rewards_za.append(r_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.zeros((num_houses, num_batches, 1))\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_za_train.append(s_t)\n",
    "        rewards_za_train.append(r_t)\n",
    "        actions_za_train.append(action)\n",
    "\n",
    "    all_states_za_train.append(np.array(states_za_train))\n",
    "    all_rewards_za_train.append(np.array(rewards_za_train))\n",
    "    all_actions_za_train.append(np.array(actions_za_train))\n",
    "    all_net_energy_za_train.append(env.mg.net_energy)\n",
    "\n",
    "# plot_results(env, all_states_za, all_rewards_za, all_actions_za, all_net_energy_za, 'Zero Agent (Family)', save=True, filename='imgs/za_family.png')\n",
    "\n",
    "print(\"Total average\",np.array(rewards_za_train, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_za_train, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "env.change_mode('eval')\n",
    "\n",
    "all_states_za_eval, all_rewards_za_eval, all_actions_za_eval, all_net_energy_za_eval = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_za_eval, rewards_za_eval, actions_za_eval= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_za_eval.append(state_0)\n",
    "    # rewards_za.append(r_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.zeros((num_houses, num_batches, 1))\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_za_eval.append(s_t)\n",
    "        rewards_za_eval.append(r_t)\n",
    "        actions_za_eval.append(action)\n",
    "\n",
    "    all_states_za_eval.append(np.array(states_za_eval))\n",
    "    all_rewards_za_eval.append(np.array(rewards_za_eval))\n",
    "    all_actions_za_eval.append(np.array(actions_za_eval))\n",
    "    all_net_energy_za_eval.append(env.mg.net_energy)\n",
    "\n",
    "# plot_results(env, all_states_za, all_rewards_za, all_actions_za, all_net_energy_za, 'Zero Agent (Family)', save=True, filename='imgs/za_family.png')\n",
    "\n",
    "print(\"Total average\",np.array(rewards_za_eval, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_za_eval, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "env.change_mode('test')\n",
    "\n",
    "all_states_za_test, all_rewards_za_test, all_actions_za_test, all_net_energy_za_test = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_za_test, rewards_za_test, actions_za_test= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_za_test.append(state_0)\n",
    "    # rewards_za.append(r_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.zeros((num_houses, num_batches, 1))\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_za_test.append(s_t)\n",
    "        rewards_za_test.append(r_t)\n",
    "        actions_za_test.append(action)\n",
    "\n",
    "    all_states_za_test.append(np.array(states_za_test))\n",
    "    all_rewards_za_test.append(np.array(rewards_za_test))\n",
    "    all_actions_za_test.append(np.array(actions_za_test))\n",
    "    all_net_energy_za_test.append(env.mg.net_energy)\n",
    "\n",
    "# plot_results(env, all_states_za, all_rewards_za, all_actions_za, all_net_energy_za, 'Zero Agent (Family)', save=True, filename='imgs/za_family.png')\n",
    "print(\"Total average\",np.array(rewards_za_test, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_za_test, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "config = load_config(\"zero_mg\")\n",
    "\n",
    "env = SimpleMicrogrid(config=config['env'])\n",
    "\n",
    "all_states_ra_train, all_rewards_ra_train, all_actions_ra_train, all_net_energy_ra_train = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_ra_train, rewards_ra_train, actions_ra_train= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_ra_train.append(state_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.random.uniform(low=-1, high=1, size=(num_houses, num_batches, 1))\n",
    "\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_ra_train.append(s_t)\n",
    "        rewards_ra_train.append(r_t)\n",
    "        actions_ra_train.append(action)\n",
    "\n",
    "    all_states_ra_train.append(np.array(states_ra_train))\n",
    "    all_rewards_ra_train.append(np.array(rewards_ra_train))\n",
    "    all_actions_ra_train.append(np.array(actions_ra_train))\n",
    "    all_net_energy_ra_train.append(env.mg.net_energy)\n",
    "\n",
    "print(\"Total average\",np.array(rewards_ra_train, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_ra_train, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "env.change_mode('eval')\n",
    "\n",
    "all_states_ra_eval, all_rewards_ra_eval, all_actions_ra_eval, all_net_energy_ra_eval = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_ra_eval, rewards_ra_eval, actions_ra_eval= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_ra_eval.append(state_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.random.uniform(low=-1, high=1, size=(num_houses, num_batches, 1))\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_ra_eval.append(s_t)\n",
    "        rewards_ra_eval.append(r_t)\n",
    "        actions_ra_eval.append(action)\n",
    "\n",
    "    all_states_ra_eval.append(np.array(states_ra_eval))\n",
    "    all_rewards_ra_eval.append(np.array(rewards_ra_eval))\n",
    "    all_actions_ra_eval.append(np.array(actions_ra_eval))\n",
    "    all_net_energy_ra_eval.append(env.mg.net_energy)\n",
    "\n",
    "\n",
    "print(\"Total average\",np.array(rewards_ra_eval, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_ra_eval, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(0)\n",
    "env.change_mode('test')\n",
    "\n",
    "all_states_ra_test, all_rewards_ra_test, all_actions_ra_test, all_net_energy_ra_test = [], [], [], []\n",
    "\n",
    "num_houses = len(env.mg.houses)\n",
    "num_batches = config['env']['batch_size']\n",
    "for _ in range(2):\n",
    "\n",
    "    states_ra_test, rewards_ra_test, actions_ra_test= [], [], []\n",
    "    \n",
    "    # Initialize states and rewards\n",
    "\n",
    "    state_0, r_0, done, _ = env.reset()\n",
    "\n",
    "    states_ra_test.append(state_0)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = np.random.uniform(low=-1, high=1, size=(num_houses, num_batches, 1))\n",
    "\n",
    "        s_t, r_t, done, _ = env.step(action)\n",
    "\n",
    "        states_ra_test.append(s_t)\n",
    "        rewards_ra_test.append(r_t)\n",
    "        actions_ra_test.append(action)\n",
    "\n",
    "    all_states_ra_test.append(np.array(states_ra_test))\n",
    "    all_rewards_ra_test.append(np.array(rewards_ra_test))\n",
    "    all_actions_ra_test.append(np.array(actions_ra_test))\n",
    "    all_net_energy_ra_test.append(env.mg.net_energy)\n",
    "\n",
    "print(\"Total average\",np.array(rewards_ra_test, dtype=float).sum(axis=0).mean(axis=1).mean())\n",
    "print(np.array(rewards_ra_test, dtype=float).sum(axis=0).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "from src.rl.a2c.d_simple_microgrid import Agent\n",
    "\n",
    "try:\n",
    "    config_eval = load_config(\"c_a2c\")\n",
    "    config_eval = config_eval['train']\n",
    "    \n",
    "    '''\n",
    "        Run the simulator\n",
    "    '''\n",
    "    set_all_seeds(0)\n",
    "\n",
    "    # Instantiate the environment\n",
    "\n",
    "    my_env = SimpleMicrogrid(config=config_eval['env'])\n",
    "\n",
    "    # Instantiate the agent\n",
    "\n",
    "    agent = Agent(\n",
    "        env=my_env, config = config_eval\n",
    "    )\n",
    "\n",
    "    # Launch the training\n",
    "\n",
    "    all_states, all_rewards, all_actions, all_net_energy = agent.train()\n",
    "\n",
    "    # Finish Wandb execution\n",
    "\n",
    "    agent.wdb_logger.finish()\n",
    "\n",
    "except (RuntimeError, KeyboardInterrupt):\n",
    "\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = np.array(all_rewards)\n",
    "all_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = all_rewards[-1,:,:,:].mean(axis=3).mean(axis=2).mean(axis=1)\n",
    "plt.plot(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = 'http://data.bayanat.ae/api/action/datastore_search?resource_id=5a8ee0ef-dfc9-4ffd-9fd8-338588137313&limit=5&q=title:jones'\n",
    "fileobj = urllib.urlopen(url)\n",
    "print (fileobj.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bcte')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4360e16d43ac692103fa28bcc8d0bd7c33534b35175524cb6d3ea499dda18b7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
